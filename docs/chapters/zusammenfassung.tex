Die Einstein-Notation hat sich zu einem mächtigen Werkzeug zur Darstellung von Tensor-Ausdrücken in vielen
wissenschaftlichen und maschinellen Lernanwendungen entwickelt. Für viele dieser Anwendungen sind
Operationen mit dünnbesetzten (spars)en Tensoren entscheidend, doch überraschenderweise gibt es nur
wenige Algorithmen, die speziell für die Lösung von Einstein-Summationsproblemen mit dünn-besetzten
Tensoren ausgelegt sind. Um diesem Mangel zu begegnen, stellen wir einen neuen, auf dem Algorithmus von
Torch basierenden Ansatz vor, der paarweise Kontraktionen auf eine Batch-Matrix-Multiplikation für
dünnbesetzte Tensoren abbildet. Wir haben unsere Methode mit drei etablierten Ansätzen verglichen: der
Sparse-Bibliothek für dünnbesetzte Tensoren, Torch und einem auf SQL basierenden Ansatz. Unsere Experimente
mit realen und synthetischen Benchmarks zeigen, dass unser Algorithmus Einstein-Summationsprobleme mit
dünnbesetzten Tensoren schneller berechnet, höherdimensionale Tensoren bewältigt und größere Probleme löst,
bei denen andere Methoden scheitern. Diese Ergebnisse unterstreichen das Potenzial unseres Algorithmus
zur Effizienzsteigerung bei der Einstein-Summation für dünnbesetzte Tensoren.