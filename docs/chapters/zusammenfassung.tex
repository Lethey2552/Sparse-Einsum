Die Einstein-Notation ist ein mächtiges Werkzeug, das vermehrt zur Darstellung von Tensor-Ausdrücken
in diversen wissenschaftlichen Kontexten sowie im maschinellen Lernen verwendet wird. Viele dieser
Anwendungen operieren auf dünnbesetzten Tensoren (auch sparse Tensoren genannt). Trotzdem existieren wenige
Algorithmen, die speziell für die Lösung von Einstein-Summationsproblemen mit dünnbesetzten Tensoren
entwickelt wurden. Der von uns in dieser Arbeit vorgestellte Algorithmus adressiert dieses Problem und
bildet paarweise Kontraktionen auf eine Batch-Matrix-Multiplikation
für dünnbesetzte Tensoren ab. Vergleiche mit Sparse, einer Bibliothek für dünnbesetzte Tensoren, Torch und
einem auf SQL basierenden Ansatz zeigen, dass unsere Methode effizienter arbeitet als diese drei bereits
etablierten Methoden. Experimente mit aus realen Problemen entnommenen sowie synthetischen Benchmarks
ergaben, dass unser Algorithmus Einstein-Summations-Probleme für dünnbesetzte Tensoren schneller berechnet
und höherdimensionale Tensoren sowie größere Probleme lösen kann als die eben genannten drei Methoden.
Diese Ergebnisse unterstreichen das Potenzial unseres Algorithmus, maßgeblich zur Effizienzsteigerung
bei Einstein-Summations-Problemen für dünnbesetzte Tensoren beitragen zu können.