% IMPORTANT: Try using a array based approach for tensor BMM instead of
% an unordered map. The array should be sorted and then contracted based
% on the indices. The unordered map is slow for large quantities of data
% because of the hasing that has to take place.

% Compared to Einsum with dense tensors, Einsum with sparse tensors has received
% little attention in the scientific community. Due to the fact that Einsum can
% be used to compute various expressions the underlying algorithms need to handle
% many different computations. Here we introduce multiple approaches and ideas,
% contributing to the field of sparse Einsum.\\
% The incorporation of machine learning and therefore linear algebra routines into
% databases has received more attention recently
% ~\cite{Machine_Learning_LinA_and_More, du2020inmachinelearningdatabasereimaginingdeep,
%     deepdive, data_management_in_machine_learning}.
% One such approach, capable of executing various tensor operations, is the mapping
% of sparse Einsum problems to SQL queries \cite{sql_einsum}. ``Efficient and Portable Einstein
% Summation in SQL" introduces four mapping rules and a decomposition scheme in which
% each large Einsum operation is split into multiple smaller Einsum operations.
% We use their work to implement our first algorithm, setting a baseline for other
% implementations to beat.
% While the earlier algorithm focused on generating SQL queries, the TACO compiler
% can compile previously known, sparse linear algebra operations into code directly
% \cite{taco}. While this produces optimized code for specific problems, it can, however,
% not handle dynamic problems due to them not being known at compile time. 

\noindent
Compared to the well-established methods for Einsum with dense tensors, Einstein Summation with
sparse tensors has received relatively little attention in the scientific community.
Due to various tensor operations that can be expressed using Einsum notation, the
underlying algorithms need to be able to handle many distinct computations. Here we
introduce multiple approaches and ideas, contributing to the field of sparse Einsum.\\
Recent developments in integrating machine learning and linear algebra routines into
databases have gained significant attention
~\cite{Machine_Learning_LinA_and_More, du2020inmachinelearningdatabasereimaginingdeep,
    deepdive, data_management_in_machine_learning}.
One such approach is the translation of sparse Einsum problems into SQL queries
\cite{sql_einsum}. The authors introduce four mapping rules and a decomposition scheme
in which large Einsum operations are split into multiple smaller Einsum operations.
In contrast to SQL-based approaches, the TACO compiler can translate known sparse linear
algebra operations into optimized code directly~\cite{taco}. While this produces
optimized code for predefined problems, it faces limitations in handling dynamic problems
that are not known at compile time.
Gray J. developed an Einsum function that calculates tensor expressions via a batch matrix
multiplication (BMM) approach~\cite{jcmgray}. This method allows for the computation of pairwise
tensor expressions by mapping them to BMMs, using summation over indices, transposition
and reshaping of the tensors. Sparse, a library designed for operations on sparse tensors,
implements an Einsum function~\cite{sparse}. However, when used alone, Sparse struggles with
large tensor expressions due to its limitations in handling a high number of different indices.
This limitation can be overcome by using Sparse as a backend for opt\_einsum, a package that
optimizes tensor contraction orders. Sparse utilizes Numba~\cite{lam2015numba} to accelerate
calculations; Numba is a just-in-time compiler that generates machine code from Python syntax.\\
In the following chapter we present two algorithms for performing Einstein summation. First,
we introduce our implementation of the four mapping rules developed in ``Efficient and Portable
Einstein Summation in SQL"~\cite{sql_einsum}, to generate SQL queries for solving Einsum problems.
This will serve as a baseline to compare other algorithms against. Second, we explain our C++
implementations with multiple levels of optimization. The underlying algorithm builds on Gray's
idea of mapping Einsum operations to a batch matrix multiplication kernel. Both algorithms decompose
large Einsum operations into smaller, pairwise operations.