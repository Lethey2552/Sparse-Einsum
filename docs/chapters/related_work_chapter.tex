% Mention jcmgrays version of mapping everything to bmm https://github.com/jcmgray/einsum_bmm/tree/main
Previous works

- SQL sparse format idea and query idea~\cite{OJBD_2019v5i1n01_Marten}\\
- jcmgrays mapping to batch matrix multiplications\\
- Sparse library using NUMBA to run einsum\\


% IMPORTANT: Try using a array based approach for tensor BMM instead of
% an unordered map. The array should be sorted and then contracted based
% on the indices. The unordered map is slow for large quantities of data
% because of the hasing that has to take place.

\noindent
Compared to Einsum with dense tensors, Einsum with sparse tensors has received
little attention in the scientific community. Due to the fact that Einsum can
be used to compute various expressions the underlying algorithms need to handle
many different computations. Here we introduce multiple approaches and ideas,
contributing to the field of sparse Einsum.\\
The incorporation of machine learning and therefore linear algebra routines into
databases has received more attention recently
~\cite{Machine_Learning_LinA_and_More, du2020inmachinelearningdatabasereimaginingdeep,
    deepdive, data_management_in_machine_learning}.
One such approach, capable of executing various tensor operations, is the mapping
of sparse Einsum problems to SQL queries \cite{sql_einsum}. ``Efficient and Portable Einstein
Summation in SQL" introduces four mapping rules and a decomposition scheme in which
each large Einsum operation is split into multiple smaller Einsum operations.
We use their work to implement our first algorithm, setting a baseline for other
implementations to beat.
While the earlier algorithm focused on generating SQL queries, the TACO compiler
can compile previously known, sparse linear algebra operations into code directly
\cite{taco}. While this produces optimized code for specific problems, it can, however,
not handle dynamic problems due to them not being known at compile time.