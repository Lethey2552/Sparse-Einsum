% \noindent
% Compared to Einsum with dense tensors, Einsum with sparse tensors has received
% little attention in the scientific community. Due to the fact that Einsum can
% be used to compute various expressions the underlying algorithms need to handle
% many different computations. Here we introduce multiple approaches and ideas,
% relevant to our algorithms or contributing to the field of sparse Einsum in general.\\
% The incorporation of machine learning and therefore linear algebra routines into
% databases has received more attention recently
% ~\cite{Machine_Learning_LinA_and_More, du2020inmachinelearningdatabasereimaginingdeep,
%     deepdive, data_management_in_machine_learning}.
% One such approach, capable of executing various sparse tensor operations, is the mapping
% of Einsum problems to SQL queries \cite{sql_einsum}. The authors introduce four mapping 
% rules and a decomposition scheme in which large Einsum operations are split into multiple 
% smaller Einsum operations. While the earlier work focused on generating SQL queries, the 
% TACO compiler can compile previously known, sparse linear algebra operations into code directly
% \cite{taco}. This produces optimized code for specific problems, however, it cannot
% handle dynamic problems due to them not being known at compile time.
Gray J. developed an Einsum function that calculates tensor expressions via a batch matrix 
multiplication (BMM) approach~\cite{jcmgray}. This method allows for the computation of pairwise 
tensor expressions by mapping them to BMMs, using summation over indices, transposition 
and reshaping of the tensors. Sparse, a library designed for operations on sparse tensors, 
implements an Einsum function~\cite{sparse}. However, when used alone, Sparse struggles with 
large tensor expressions due to its limitations in handling a high number of different indices. 
This limitation can be overcome by using Sparse as a backend for opt\_einsum, a package that 
optimizes tensor contraction orders. Sparse utilizes Numba~\cite{lam2015numba} to accelerate 
calculations; Numba is a just-in-time compiler that generates machine code from Python syntax.\\
In the following chapter we present two algorithms for performing Einstein summation. First, 
we introduce our implementation of the four mapping rules developed in ``Efficient and Portable 
Einstein Summation in SQL"~\cite{sql_einsum}, to generate SQL queries for solving Einsum problems. 
This will serve as a baseline to compare other algorithms against. Second, we explain our C++ 
implementations with multiple levels of optimization. The underlying algorithm builds on Gray's 
idea of mapping Einsum operations to a batch matrix multiplication kernel. Both algorithms decompose
large Einsum operations into smaller, pairwise operations.