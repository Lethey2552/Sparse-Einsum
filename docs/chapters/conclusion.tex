We have constructed a new algorithm to solve Einstein summation problems for sparse tensor networks.
Our algorithm maps the pairwise contraction operations onto a sparse batch matrix multiplication kernel
by preprocessing and postprocessing the tensors. We have compared our methods against the commonly used
and supported Einsum library Sparse~\cite{sparse}, the well known machine learning library
Torch~\cite{pytorch}, and an approach that maps Einsum problems to SQL. Our experiments involved
real-world problems and generated Einsum expressions that mimic a wide range of tensor properties.
In doing so, we found that our approach can solve sparse Einstein summation problems in less time,
support tensors with higher dimensionality than other specialized libraries and compute larger problems
where other approaches fail. In the future, we plan to further improve the memory efficiency and
memory access pattern of our C++ implementation, streamline the preprocessing and improve upon the
sparse batch matrix multiplication kernel.