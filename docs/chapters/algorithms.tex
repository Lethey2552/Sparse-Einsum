In this chapter we present two algorithms for performing Einstein summation. First,
we introduce our implementation of the four mapping rules developed in ``Efficient and Portable
Einstein Summation in SQL"~\cite{sql_einsum}, to generate SQL queries for solving Einsum problems.
This will serve as a baseline to compare other algorithms against. Second, we explain our C++
implementations with multiple levels of optimization. The underlying algorithm of the C++
implementations builds on Torch's strategy of mapping Einsum operations to a batch matrix
multiplication kernel. Both algorithms, namely the algorithm for the SQL implementation and
the algorithm used for the C++ versions, decompose large Einstein summation operations
into smaller, pairwise operations to exploit efficient contraction paths.

\section{The SQL Algorithm}
In this section, we present Blacher et al.'s~\cite{sql_einsum} algorithm and our
implementation of it for mapping format strings and the corresponding tensors to SQL,
enabling Einstein summation in databases. First, we introduce the portable schema for
representing tensors, specifically sparse tensors, in SQL. We then show their four mapping
rules to generate non-nested Einsum queries from arbitrary format strings. Finally, we
explain how we exploit efficient contraction paths by decomposing large Einsum queries
into smaller parts.

\subsection{Portable Schema for Tensors}
Blacher et al. chose the COO format to represent tensors as it only uses integers and
floating point numbers, which results in a vendor independent schema for encoding tensors
across various database management systems (DBMS). For example, a 3D tensor $A \in
    \mathbb{R}^{I \times J \times K}$ has the following schema:

\[
    A(i\ \ INT,\ \ j\ \ INT,\ \ k\ \ INT,\ \ val\ \ DOUBLE)
\]
%
Each tensor is stored in a separate table. In the example, table A stores a 3D tensor,
where each value ($val$) can be addressed by specifying the corresponding indices
($i$,~$j$,~$k$).

\subsection{Mapping Einstein Summation to SQL}
``Efficient and Portable Einstein Summation in SQL" introduces four rules for mapping
any tensor expression in Einstein notation to SQL.

\begin{description}[leftmargin=!,labelwidth=\widthof{\bfseries R1}]
    \item [R1] All input tensors are enumerated in the FROM clause.
    \item [R2] The indices of the output tensor are enumerated in the SELECT clause
          and the GROUP BY clause.
    \item [R3] The new value is the SUM of all values multiplied together.
    \item [R4] Indices that are the same among input tensors are transitively equated
          in the WHERE clause.
\end{description}
%
Say we want to map the tensor operation given by $ik,k \rightarrow i$, a matrix-vector
multiplication, with tensors $A \in \mathbb{R}^{I \times K}$, $v \in \mathbb{R}^{K}$ and

\begin{equation*}
    A =
    \begin{bmatrix}
        0.0 & 1.0 \\
        0.0 & 0.0 \\
        5.0 & 0.0 \\
        0.0 & 0.0
    \end{bmatrix},
    \quad
    v =
    \begin{bmatrix}
        4.0 \\
        1.0
    \end{bmatrix}.
\end{equation*}
%
When applying all four rules to map the example tensor expression to SQL, we get the result
seen in Listing \ref{lst:einsum:sql}.

\begin{lstlisting}[caption={Einstein summation in SQL.}, captionpos={t}, label={lst:einsum:sql}]
    WITH A(i, j, val) AS (                          -- matrix A
        VALUES (0, 1, 1.0), (2, 0, 5.0)
    ), v(i, val) AS (                               -- vector v
        VALUES (0, 4.0), (1, 1.0)
    ) SELECT A.i AS i,                              -- R2
        SUM(A.val * v.val) AS val                   -- R3
      FROM A, v                                     -- R1
      WHERE A.j=v.i                                 -- R4
      GROUP BY A.i                                  -- R2
\end{lstlisting}
%
While all four rules are needed to ensure every possible Einstein summation problem
can be translated to SQL, for some tensor expressions the conditions to apply the rules
R2 and/or R4 are not fulfilled. If there are no indices after the arrow in the format
string, the output is scalar and does not require R2. Furthermore, if there are no common
indices among the input tensors, there is no summation in the tensor expression and R4
can be omitted. The rules guarantee a correct mapping, not a mapping with minimal code size.
Though requirering extra checks, in some cases the SQL queries could be simplified further.

\subsection{Optimizing Contraction Order}
Mapping a tensor expression directly into a single, non-nested SQL query in Einstein
notation is known to produce execution times that are far from optimal, especially for operations
involving many tensors. The inefficiency stems from the fact that conventional query optimizers
are unaware of the contraction order of the repeating indices within tensor expressions and are
therefore incapable of effectively breaking down the query into smaller parts to exploit efficient
contraction paths as described in the Background section \ref{sec:tensor:contractions}.\\
One can get around this using intermediate tensors via subqueries or common table expressions,
which decomposes one large Einstein summation query into smaller pieces and lets the database
engine follow a pre-defined contraction order. More precisely, using GROUP BY and SUM
aggregation in intermediate computations enforce query engines to evaluate the query in the
right order.

% TODO: Edit this section to list the right arguments for calling sql_einsum_query()
\section{Our Implementation of the SQL Algorithm}
We implemented the algorithm for mapping Einsum format strings to SQL queries proposed by Blacher
et al. in Python 3.11.0 as a small package, only requiring Numpy as a dependency. When
calling \textit{sql\_einsum\_query()}, an Einsum notation string, the tensor names, and the tensor
data have to be supplied. The path argument is optional. When not supplied with a path, an
optimized contraction path is calculated using cgreedy~\cite{cgreedy}. The cgreedy package
provides a greedy algorithm approach for finding an efficient contraction order for any given
format string and associated tensors, utilizing multiple cost functions. The construction of the
query is separated into two parts. The first part creates the tensors in COO format as SQL
compatible structures and returns the appropriate query. The second part applies the decomposition
schema, more precisely, it uses either the supplied or the calculated contraction path to build a
contraction list. The entries of the contraction list dictate the order and the exact pairwise
operations necessary to solve the Einstein summation problem. These subproblems are also specified
in Einstein notation. To build the second part of the query, we iterate the contraction list and
apply the four mapping rules explained in the previous section to assemble the correct SQL
strings for the given pairwise contractions. Finally, we merge the two generated query parts and
return the complete query.

\section{The C++ Algorithm}
Our method expands on Torch's strategy of mapping the Einstein summation problems to batch matrix
multiplication (BMM). We give an overview of our approach that pre-processes tensors for pairwise
operations, calculates the result via BMM and finally post-processes the result to fit the expected
output format. This separation of the computational parts into three distinct steps allows for the
simple application of the decomposition schema and enables independent debugging, measureing and
improveing of each section, while giving the benefit of readability. We will explain the sparse
BMM first to introduce the template that has to be followed by the pre-processing.

\subsection{Pre-Processing}
The pre-processing phase of the algorithm is critical for aligning the tensors in Coordinate List
(COO) format with the requirements of batch matrix multiplication. In order to achieve predictable
computations with the BMM, we apply the template seen in Figure \ref{fig:bmm:template}. The indices
of the two input tensors are grouped as follows:

% TODO: Right tensor index s_l is wrong in the figure. Change it to s_r.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.7]{gray_bmm_indice_grouping.pdf}
    \caption{The figure used by Gray J. \cite{jcmgray}  to describe the classification of indices
        used for the grouping. The left tensor has the index order $(b, k_l, c, s_l)$ and the right
        tensor $(b, c, k_r, s_r)$.}
    \label{fig:bmm:template}
\end{figure}
%
\begin{description}[leftmargin=!,labelwidth=\widthof{\bfseries Contracted Indices (c)}]
    \item [Batch Indices (b)] All individual batch dimensions have to be combined into a single
          batch dimension.
    \item [Contracted Indices (c)] The dimensions over which the contraction takes place.
    \item [Summed Indices (s)] All indices that only have a single occurence. This is grouped for
          both input tensors separately.
    \item [Kept Indices (k)] All indices and their dimensions that occur in only a single input
          tensor and in the output. This is grouped for both input tensors separately.
\end{description}
%
For our implementation we treat the removal of the summed indices as part of the preprocessing.
This means the input tensors for the BMM only have index order $(b, k_l, c)$ for the left term and
$(b, c, k_r)$ for the right term. For the COO format, since every index results in a column,
the index order translates to the columns, followed by a last column for the value. To enable the use
of this format we apply multiple pre-processing steps. First, the algorithm calculates appropriate
format strings for the COO tensors. The format strings should align with the requirements of the BMM
to ensure correctness and efficiency of the subsequent operation.\\
Besides computing the format strings, the algorithm computes the new shapes for the tensors.
In doing so, this step will combine all batch dimensions into one and ignore dimensions not present
in the format string. Precise computation of these new shapes is critical to ensure that the tensors
are aligned correctly for the BMM.\\
The computed format strings are used to call a special, single Einsum function. This function performs
Einstein summation on a single tensor, allowing for the computation of diagonals, summation over
specified dimensions and the permutation thereof. The new shapes are used to reshape the tensors to
comply with the dimensional requirements of BMM. Both of these steps are only performed if necessary.
The pre-processing will result in a COO tensor, which is suited for batch matrix multiplication.

\subsection{Sparse BMM}
The BMM computes the results in batches. Say $A$ and $B$ are matrices within the same batch with indices
$(k_l, c)$ for $A$ and indices $(c, k_r)$ for $B$. The pairwise matrix multiplication for the COO
matrices is performed using the following algorithm:

\begin{enumerate}[label*=\arabic*.]
    \item Transpose $B$ by swapping the index columns.
    \item Sort the rows of $B$ by comparing the index values from first to last.
    \item Traverse both matrices $A$ and $B$ row-wise and
          \begin{enumerate}[label*=\arabic*.]
              \item Compare 2nd element of the current row in $A$ and $B$.
              \item If they match, store a new row in the resulting COO matrix with the 1st element of
                    $A$'s current row as its 1st element, the 1st element of $B$'s current row as its
                    2nd element and the product of the two rows values.
          \end{enumerate}
    \item If there are any two rows with the same indices in the resulting matrix, add their values and
          store them with the previously duplicate indices.
    \item Sort the resulting matrix.
\end{enumerate}
The final tensor is returned and can now be post-processed.

\subsection{Post-Processing}
The result of the BMM may have to be post-processed to fit the contraction lists output
specifications. The post-processing includes the reshaping of the single, combined batch dimension
into the specified number of batch dimensions by treating it as a multi-index and unraveling it.
Furthermore, the final dimensions may be permuted by swapping the COO tensors columns to fit the
correct output format. Again, both of these steps are only performed if necessary.